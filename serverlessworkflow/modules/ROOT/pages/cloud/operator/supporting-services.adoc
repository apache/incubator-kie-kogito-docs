= Supporting Services
:compat-mode!:
// Metadata:
:description: Deploy Supporting Services with {operator_name}
:keywords: kogito, sonataflow, workflow, serverless, operator, kubernetes, openshift, containers, data, index, job, service, cluster, wide, scoped, cluster-wide, cluster-scoped
// links
:kogito_serverless_operator_url: https://github.com/apache/incubator-kie-kogito-serverless-operator/


This document describes how to configure and deploy the {product_name}'s xref:data-index/data-index-core-concepts.adoc[Data Index] and xref:job-services/core-concepts.adoc[Job Service] supporting services, using the {operator_name}.

In general, in a regular {product_name} installation you must deploy both services to ensure a successful execution of your workflows. To get more information about each service please read the respective guides.

.Prerequisites
* The {operator_name} installed. See xref:cloud/operator/install-serverless-operator.adoc[] guide.
* A PostgreSQL database service instance. Required if you are planning to use the <<postgresql-persistence-configuration>> for a supporting service.

[#supporting-services-workflow-communications]
== Supporting Services and Workflow communications

When you deploy a supporting service in a given namespace, you can do it by using an <<enabled-deployment-field, enabled or disabled>> deployment.

An enabled deployment, signals the {operator_name} to automatically intercept every workflow deployment with the `preview` or `gitops` profile, in this namespace, and automatically configure it to connect with that service.

For example, if the Data Index is enabled, a workflow will be automatically configured to send workflow status change events to it.
And, similar configurations are produced if the Job Service is enabled, to create a Job, every time a workflow requires a timeout.
Additionally, the operator will configure the Job Service to send events to the Data Index Service, etc.

As you can see, the operator can not only deploy a supporting service, but also, manage other configurations to ensure the successful execution of a workflow.

Fortunately, all these configurations are managed automatically, and you must only provide the supporting services configuration in the `SonataFlowPlatform` CR.

[NOTE]
====
Scenarios where you only deploy one of the supporting services, or configure a disabled deployment, are intended for advanced use cases.
In a regular installation, you must normally configure an enabled deployment of both services to ensure a successful execution of your workflows.
====

[#deploy-supporting-services]
== Deploying the supporting services using the `SonataFlowPlatform` CR

To deploy the supporting services you must use the sub-fields `dataIndex` and `jobService` in the `SonataFlowPlatform` CR `spec.services`.
That information signals the {operator_name} to deploy each service when the `SonataFlowPlatform` CR is deployed.

[NOTE]
====
Each service configuration is considered independently, and you can combine these configurations with any other configuration present in the `SonataFlowPlatform` CR.
====

The following `SonataFlowPlatform` CR fragment shows a scaffold configuration that you can use as reference:
[#supporting-services-configuration]
.Supporting services configuration
[source,yam]
----
apiVersion: sonataflow.org/v1alpha08
kind: SonataFlowPlatform
metadata:
  name: sonataflow-platform-example
  namespace: example-namespace
spec:
  services:
    dataIndex: <1>
      enabled: true <2>
      # Specific configurations for the Data Index Service
      # might be included here
    jobService: <3>
      enabled: true <4>
      # Specific configurations for the Job Service
      # might be included here
----
[#enabled-deployment-field]
<1> Data Index Service configuration field.
<2> If true, produces an enabled Data Index Service deployment, <<supporting-services-workflow-communications, see>>. Other cases produce a disabled deployment. The default is `false`.
<3> Job Service configuration field.
<4>  If true, produces an enabled Job Service deployment, <<supporting-services-workflow-communications, see>>. Other cases produce a disabled deployment. The default is `false`.

[NOTE]
====
The configuration above produces an ephemeral deployment of each service, <<ephemeral-persistence-configuration, see>>.
====

== Supporting Services Scope

The `SonataFlowPlatform` CR facilitates the deployment of the supporting services with namespace scope.
It means that, all the automatically configured <<supporting-services-workflow-communications>>, are restricted to the namespace of the given platform.
This can be useful, in cases where you need separate supporting service instances for a set of workflows.
For example, a given application can be deployed isolated with its workflows, and the supporting services.

Additionally, using the `SonataFlowClusterPlatform` CR it's possible to configure a <<cluster-scoped-deployment, cluster scoped deployment>> of the supporting services.

== Configuring the Supporting Services Persistence

[#ephemeral-persistence-configuration]
=== Ephemeral persistence configuration

The ephemeral persistence of a service is supported by an embedded PostgreSQL database dedicated to it. That database is re-created by the operator on every service restart.
And thus, it's only recommended for development and testing purposes.

The ephemeral deployment of a service requires no additional configurations than the shown, <<supporting-services-configuration, here>>.

[#database-migration-configuration]
=== Database migration configuration

The term database migration refers to either initializing a given Data Index or Jobs Service database to the respective schema, or applying data or schema updates as new versions are released.

The database migration strategy must be configured individually for each supporting service, by using the optional  `dataIndex.persistence.dbMigrationStrategy` and `jobService.persistence.dbMigrationStrategy` fields respectively.

You can use any of the following database migration strategies: `job`, `service`, and `none`. When not configured, the default value is `service`.

[NOTE]
====
Currently, database migration is only supported when using the PostgreSQL persistence configuration.
====

==== Job based database migration strategy

When you configure the `job` based strategy, the {product_name} Operator decouples the database migration by using a dedicated Kubernetes Job that manages the entire migration process.

That Job executes before the supporting services deployment, and thus, ensures that a given service is started only if the corresponding migration was successful.

You would typically use this strategy in production environments.

==== Service based database migration strategy

When you configure the `service` based strategy, the database migration is managed by each supporting service and is executed as part of the service's startup sequence.
In worst-case scenarios, a service could start with failures.

==== None migration strategy

When you configure the `none` strategy, no Operator nor service migration attempt is executed.

You would typically use this strategy in environments where a database administrator executes all the database migrations.

[#postgresql-persistence-configuration]
=== PostgreSQL persistence configuration

The PostgreSQL persistence of a service is supported by a PostgreSQL server instance that you must have previously installed on the cluster.
The administration of that instance is totally independent of the {operator_name} scope, and to connect a supporting service with it, you must only configure the correct database connection parameters.

The following `SonataFlowPlatform` CR fragment shows the configuration options that you must use:

.PostgreSQL persistence configuration
[source,yaml]
----
apiVersion: sonataflow.org/v1alpha08
kind: SonataFlowPlatform
metadata:
  name: sonataflow-platform-example
  namespace: example-namespace
spec:
  services:
    dataIndex:
      enabled: true
      persistence:
        dbMigrationStrategy: job <1>     
        postgresql:
          serviceRef:
            name: postgres-example <2>
            namespace: postgres-example-namespace <3>
            databaseName: example-database <4>
            databaseSchema: data-index-schema <5>
            port: 1234 <6>
          secretRef:
            name: postgres-secrets-example <7>
            userKey: POSTGRESQL_USER <8>
            passwordKey: POSTGRESQL_PASSWORD <9>
    jobService:
      enabled: true
      persistence:
        dbMigrationStrategy: job
        postgresql:
        # Specific database configuration for the Job Service
        # might be included here.
----
<1> (Optional) Database migration strategy to use, <<database-migration-configuration, see>>. Defaults to `service`.
<2> Name of the Kubernetes Service to connect with the PostgreSQL database server.
<3> (Optional) Kubernetes namespace containing the PostgreSQL Service. Defaults to the `SonataFlowPlatform's` local namespace.
<4> Name of the PostgreSQL database to store the supporting service data.
<5> (Optional) Name of the PostgreSQL database schema to store the supporting service data.
Defaults to the `SonataFlowPlatform's` `name`, suffixed with `-data-index-service` or `-jobs-service`. For example, `sonataflow-platform-example-data-index-service`.
<6> (Optional) Port number to connect with the PostgreSQL Service. Defaults to 5432.
<7> Name of the link:{k8n_secrets_url}[Kubernetes Secret] containing the username and password to connect with the database.
<8> Name of the link:{k8n_secrets_url}[Kubernetes Secret] `key` containing the username to connect with the database.
<9> Name of the link:{k8n_secrets_url}[Kubernetes Secret] `key` containing the password to connect with the database.

[NOTE]
====
The persistence of each service can be configured independently by using the respective `persistence` field.
====

To create the secrets for the example above you can use a command like this:

.Create secret example
[source,bash]
----
kubectl create secret generic postgres-secrets-example  --from-literal=POSTGRESQL_USER=<user> --from-literal=POSTGRESQL_PASSWORD=<password> -n postgres-example-namespace
----


[#common-persistence-configuration]
=== Platform-scoped PostgreSQL persistence configuration

To configure a common PostgreSQL service and database for the supporting services, you must use the field `spec.persistence.postgresql` in the `SonataFlowPlatform` CR.

This information signals the {operator_name} to configure the supporting services to connect to that database.

Additionally, workflows deployed in that namespace, with the `preview` or `gitops` profile, that don’t provide a custom persistence configuration, will connect to that database too, xref:cloud/operator/using-persistence.adoc#configuring-persistence-using-the-sonataflowplatform-cr[see].

And, similarly to the workflow's persistence, the following rules apply:

* If a supporting service has a configured persistence, for example, the field `services.dataIndex.persistence.postgresql` is configured. That configuration will apply.

* If a supporting service has no configured persistence, for example, if the `services.dataIndex.persistence.postgresql` field is not set, the persistence configuration will be taken from the current platform.

* Additionally, if a supporting service requires a particular database migration strategy, you must use the respective `dataIndex.persistence.dbMigrationStrategy` and `jobService.persistence.dbMigrationStrategy` fields, <<#database-migration-configuration, see>>.

The following `SonataFlowPlatform` CR fragment shows the configuration options that you must use:

.SonataFlowPlatform CR persistence configuration example
[source,yaml]
----
apiVersion: sonataflow.org/v1alpha08
kind: SonataFlowPlatform
metadata:
  name: sonataflow-platform-example
  namespace: example-namespace
spec:
  persistence:
    postgresql:
      serviceRef:
        name: postgres-example <1>
        namespace: postgres-example-namespace <2>
        databaseName: example-database <3>
        port: 1234 <4>
      secretRef:
        name: postgres-secrets-example <5>
        userKey: POSTGRESQL_USER <6>
        passwordKey: POSTGRESQL_PASSWORD <7>
    dataIndex:
      enabled: true
      persistence:
        dbMigrationStrategy: job <8>
    jobService:
      enabled: true
      persistence:
        dbMigrationStrategy: service <9>
----

<1> Name of the Kubernetes Service to connect with the PostgreSQL database server.
<2> (Optional) Kubernetes namespace containing the PostgreSQL Service. Defaults to the `SonataFlowPlatform's` local namespace.
<3> Name of the PostgreSQL database to store the supporting services and workflows data.
<4> (Optional) Port number to connect with the PostgreSQL Service. Defaults to 5432.
<5> Name of the link:{k8n_secrets_url}[Kubernetes Secret] containing the username and password to connect with the database.
<6> Name of the link:{k8n_secrets_url}[Kubernetes Secret] `key` containing the username to connect with the database.
<7> Name of the link:{k8n_secrets_url}[Kubernetes Secret] `key` containing the password to connect with the database.
<8> (Optional) Database migration strategy to use for the Data Index, <<database-migration-configuration, see>>. Defaults to `service`.
<9> (Optional) Database migration strategy to use for the Job Service Index, <<database-migration-configuration, see>>. Defaults to `service`. While no usual, the supporting services could use distinct database migration strategies.

[NOTE]
====
When you use the common PostgreSQL configuration, the database schema for each supporting service is automatically configured as the SonataFlowPlatform’s `name`, suffixed with `-data-index-service` or `-jobs-service` respectively.
For example, `sonataflow-platform-example-data-index-service`.
====

[#configuring-supporting-services-eventing-system]
== Configuring the supporting services Eventing system

In general, the following events are produced in a {product_name} installation:

* Workflow outgoing and incoming business events.
* {product_name} system events sent from the workflow to the Data Index and Job Service respectively.
* {product_name} system events sent from the Jobs Service to the Data Index Service.

The {operator_name} is designed to use the link:{knative_eventing_url}[Knative Eventing] system to resolve all the event communication between these services.

[NOTE]
====
In a regular {product_name} installation, the preferred method is to use the <<platform-scoped-eventing-system-configuration, Platform scoped Eventing System Configuration>>, while the <<service-scoped-eventing-system-configuration, Service scoped Eventing System configuration>> is reserved only for advanced use cases.
====

[#platform-scoped-eventing-system-configuration]
=== Platform-scoped Eventing system configuration

To configure a platform-scoped eventing system, you must use the field `spec.eventing.broker.ref` in the `SonataFlowPlatform` CR to refer to a Knative Eventing Broker.

This information signals the {operator_name} to automatically link the supporting services to `produce` and `consume` the events by using that Broker.

Additionally, workflows deployed in that namespace, with the `preview` or `gitops` profile, that don't provide a custom eventing system configuration, will be linked to that Broker.
For more information about configuring the workflow eventing system, xref:cloud/operator/configuring-workflow-eventing-system.adoc[see].

The following `SonataFlowPlatform` CR fragment shows an example of such configuration:

.Platform scoped eventing system configuration example
include::common/platform-scoped-eventing-system-configuration-example.adoc[]


[#service-scoped-eventing-system-configuration]
=== Service-scoped Eventing system configuration

A service scoped eventing system configuration provides the ability to do a fine-grained configuration of the Eventing system for the <<data-index-eventing-system-configuration, Data Index>> or the <<jos-service-eventing-system-configuration, Jobs Service>>.

[NOTE]
====
In a regular {product_name} installation, the preferred method is to use a <<platform-scoped-eventing-system-configuration, Platform-scoped Eventing System Configuration>>, while the service-scoped configuration is reserved only for advanced use cases.
====

[#data-index-eventing-system-configuration]
=== Data Index Eventing system configuration

To configure a service-scoped eventing system for the Data Index, you must use the field `spec.services.dataIndex.source.ref` in the `SonataFlowPlatform` CR to refer to a specific Knative Eventing Broker.

This information signals the {operator_name} to automatically link the Data Index to `consume` the {product_name} system events from that Broker.

.Data Index service scoped eventing system configuration example
[source,yam]
----
apiVersion: sonataflow.org/v1alpha08
kind: SonataFlowPlatform
metadata:
  name: sonataflow-platform-example
spec:
  services:
    dataIndex:
      source:
        ref:
          name: data-index-source-example-broker <1>
          namespace: data-index-source-example-broker-namespace <2>
          apiVersion: eventing.knative.dev/v1
          kind: Broker
----

<1> Name of the Knative Eventing Broker to `consume` events from.
<2> Optional: Defines the namespace of the Knative Eventing Broker. Defaults to the SonataFlowPlatform namespace. In general, we recommend creating the Knative Eventing Broker in the same namespace as the SonataFlowPlatform.

[NOTE]
====
In production environments, you must use a production-ready broker, like the link:{knative_eventing_kafka_broker_url}[Knative Kafka Broker].
====

[#jos-service-eventing-system-configuration]
=== Jobs Service Eventing system configuration

To configure a service-scoped eventing system for the Jobs Service, you must use the fields `spec.services.jobService.source.ref` and `spec.services.jobService.sink.ref` in the `SonataFlowPlatform` CR.

This information signals the {operator_name} to automatically link the Jobs Service to `consume` and `produce` the {product_name} system events from that configuration respectively.

.Jobs Service scoped eventing system configuration example
[source,yam]
----
apiVersion: sonataflow.org/v1alpha08
kind: SonataFlowPlatform
metadata:
  name: sonataflow-platform-example
spec:
  services:
    jobService:
      source:
        ref:
          name: jobs-service-source-example-broker <1>
          namespace: jobs-service-source-example-broker-namespace <2>
          apiVersion: eventing.knative.dev/v1
          kind: Broker
      sink:
        ref:
          name: jobs-service-sink-example-broker <3>
          namespace: jobs-service-sink-example-broker-namespace <4>
          apiVersion: eventing.knative.dev/v1
          kind: Broker
----

<1> Name of the Knative Eventing Broker to `consume` events from.
<2> Optional: Defines the namespace of the Knative Eventing Broker. Defaults to the SonataFlowPlatform namespace. In general, we recommend creating the Knative Eventing Broker in the same namespace as the SonataFlowPlatform.
<3> Name of the Knative Eventing Broker to `produce` events on.
<4> Optional: Defines the namespace of the Knative Eventing Broker. Defaults to the SonataFlowPlatform namespace. In general, we recommend creating the Knative Eventing Broker in the same namespace as the SonataFlowPlatform.

[NOTE]
====
In production environments, you must use production-ready brokers, like the link:{knative_eventing_kafka_broker_url}[Knative Kafka Broker].
====

[#cluster-scoped-eventing-system-configuration]
=== Cluster-scoped Eventing system configuration

When you use a <<cluster-scoped-deployment>> deployment, the supporting services are automatically linked to the `Broker` configured in the `SonataFlowPlatform` CR referred to by the `SonataFlowClusterPlatform` CR.

=== Eventing system configuration precedence rules

To configure the eventing system for a supporting service, the {operator_name} use the following precedence rules:

. If the supporting service has a configured eventing system, by using any of the <<data-index-eventing-system-configuration>> or <<jos-service-eventing-system-configuration>> respectively, that configuration applies.

. If the `SonataFlowPlatform` CR enclosing the supporting service, is configured with a <<platform-scoped-eventing-system-configuration>>, that configuration applies.

. If the current cluster, is configured with a <<cluster-scoped-eventing-system-configuration>>, that configuration apply.

. If none of the previous configurations exists, the supporting service is configured to produce direct HTTP calls to deliver events.

=== Eventing System linking objects

The linking of the supporting services with the Eventing System is produced by using Knative Eventing SinkBindings and Triggers.
These objects are automatically created by the {operator_name}, and facilitate the events production and consumption from the supporting services.

The following example shows the Knative Native eventing objects created for the `SonataFlowPlatform` CR:

.SonataFlowPlatform eventing system configuration example
[source,yaml]
----
apiVersion: sonataflow.org/v1alpha08
kind: SonataFlowPlatform
metadata:
  name: sonataflow-platform-example
  namespace: example-namespace
spec:
  eventing:
    broker:
      ref:
        name: example-broker <1>
        apiVersion: eventing.knative.dev/v1
        kind: Broker
  services:
    dataIndex: <2>
      enabled: true
    jobService: <3>
      enabled: true
----

<1> Platform Broker configuration used by the Data Index, Jobs Service, and the workflows if not overridden.
<2> Data Index ephemeral deployment.
<3> Jobs Service ephemeral deployment.

.Knative Kafka Broker example used by the SonataFlowPlatform
[source,yaml]
----
apiVersion: eventing.knative.dev/v1
kind: Broker
metadata:
  annotations:
    eventing.knative.dev/broker.class: Kafka <1>
  name: example-broker
  namespace: example-namespace
spec:
  config:
    apiVersion: v1
    kind: ConfigMap
    name: kafka-broker-config
    namespace: knative-eventing
----

<1> Use the Kafka class to create a Kafka Knative Broker

.Knative Eventing Triggers created for the Data Index and Jobs Service events consumption
[source,bash]
----
kn trigger list -n example-namespace

NAME                                                              BROKER           SINK                                                     AGE    CONDITIONS   READY   REASON
data-index-jobs-fbf285df-c0a4-4545-b77a-c232ec2890e2              example-broker   service:sonataflow-platform-example-data-index-service   106s   7 OK / 7     True
data-index-process-definition-e48b4e4bf73e22b90ecf7e093ff6b1eaf   example-broker   service:sonataflow-platform-example-data-index-service   106s   7 OK / 7     True
data-index-process-error-fbf285df-c0a4-4545-b77a-c232ec2890e2     example-broker   service:sonataflow-platform-example-data-index-service   106s   7 OK / 7     True
data-index-process-instance-mul35f055c67a626f51bb8d2752606a6b54   example-broker   service:sonataflow-platform-example-data-index-service   106s   7 OK / 7     True
data-index-process-node-fbf285df-c0a4-4545-b77a-c232ec2890e2      example-broker   service:sonataflow-platform-example-data-index-service   106s   7 OK / 7     True
data-index-process-sla-fbf285df-c0a4-4545-b77a-c232ec2890e2       example-broker   service:sonataflow-platform-example-data-index-service   106s   7 OK / 7     True
data-index-process-state-fbf285df-c0a4-4545-b77a-c232ec2890e2     example-broker   service:sonataflow-platform-example-data-index-service   106s   7 OK / 7     True
data-index-process-variable-ac727d6051750888dedb72f697737c0dfbf   example-broker   service:sonataflow-platform-example-data-index-service   106s   7 OK / 7     True

jobs-service-create-job-fbf285df-c0a4-4545-b77a-c232ec2890e2      example-broker   service:sonataflow-platform-example-jobs-service         106s   7 OK / 7     True
jobs-service-delete-job-fbf285df-c0a4-4545-b77a-c232ec2890e2      example-broker   service:sonataflow-platform-example-jobs-service         106s   7 OK / 7     True
----

.Knative Eventing SinkBinding created for the Jobs Service events production
[source,bash]
----
kn source list -n example-namespace

NAME                                          TYPE          RESOURCE                           SINK                    READY
sonataflow-platform-example-jobs-service-sb   SinkBinding   sinkbindings.sources.knative.dev   broker:example-broker   True
----

== Configuring Scaling and Disruption Protection

=== Configuring Auto Scaling

Currently, only the Data Index supports auto-scaling configuration.

To configure the auto-scaling, you must create an external `HorizonalPodAutoscaler` that targets the given Data Index service `Deployment`.

When the {operator_name} detects a `HorizontalPodAutoscaler` in the namespace targeting the Data Index `Deployment`, replica management is delegated to the HPA controller. If the `HorizontalPodAutoscaler` is removed, the {operator_name} controller resumes control of replica management.

To calculate the Data Index `Deployment` name, you must follow this rule:

* Concatenate the name of the `SonataFlowPlatform`, with the suffix `-data-index-service`.
For the example below, the calculated name is: `sonataflow-platform-example-data-index-service`.

.SonataFlowPlatform CR  example
[source,yaml]
----
apiVersion: sonataflow.org/v1alpha08
kind: SonataFlowPlatform
metadata:
  name: sonataflow-platform-example
  namespace: example-namespace
spec:
  services:
    dataIndex:
      enabled: true
    jobService:
      enabled: true
# Only a fragment of the SonataFlowPlatform is shown for simplicity
----

The following example shows a `HorizontalPodAutoscaler` configuration for the `sonataflow-platform-example-data-index-service` `Deployment`:

.Example of HorizontalPodAutoscaler configuration that targets a Data Index `Deployment`
[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: data-index-example-autoscaler <1>
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: sonataflow-platform-example-data-index-service <2>
  minReplicas: 3
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu <3>
        target:
          type: Utilization
          averageUtilization: 70
----
<1> Any name of your choice.
<2> The calculated name of the Data Index `Deployment`. The other fields of the `scaleTargetRef` configuration remains un changed.
<3> Example of a metric based on the cpu resource utilization.

To configure the `HorizontalPodAutoscaler` metrics, you can use any of the resources automatically configured by the {operator_name}. Alternatively, to modify the default values <<advanced_supporting_services_configuration,see>>.

.Default values configured by the {operator_name} for the Data Index `Deployment`
[source, yaml]
----
    - resources:
        limits:
          cpu: 200m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 1Gi
----

[#advanced_supporting_services_configuration]
== Advanced Supporting Services Configurations

To configure the advanced options for any of the supporting services you must use the `podTemplate` field respectively, for example `dataIndex.podTemplate`:

.Advanced configurations example for the Data Index Service.
[source,yaml]
----
apiVersion: sonataflow.org/v1alpha08
kind: SonataFlowPlatform
metadata:
  name: sonataflow-platform-example
  namespace: example-namespace
spec:
  services:
    dataIndex:
      enabled: true
      podTemplate:
        replicas: 2 <1>
        container: <2>
          env: <3>
            - name: ANY_ADVANCED_CONFIG_PROPERTY
              value: any-value
          image: <4>
          resources:
            limits:
              cpu: <5>
              memory: <6>
            requests:
              cpu: <7>
              memory: <8>
        initContainers: <9>
----

<1> Number of replicas. Defaults to 1. In the case of the jobService this value is always overridden to 1 by the operator, since that service is a singleton service.
<2> Holds the particular configurations for the container that will execute the given supporting service.
<3> Standard Kubernetes `env` configuration. This can be useful in cases where you need to fine tune any of the supporting services properties.
<4> Standard Kubernetes `image` configuration. This can be useful in cases where you need to use an updated image for any of the supporting services.
<5> Standard Kubernetes `cpu` limit configuration. Defaults to 200m.
<6> Standard Kubernetes `memory` limit configuration. Defaults to 1Gi.
<7> Standard Kubernetes `cpu` request configuration. Defaults to 100m.
<8> Standard Kubernetes `memory` request configuration. Defaults to 1Gi.
<9> Standard Kubernetes `initContainers` for the Pod that executes the supporting service.

[NOTE]
====
The `podTemplate` field of any supporting service has the majority of fields defined in the default Kubernetes PodSpec API.
The same Kubernetes API validation rules apply to these fields.
====

[#cluster-scoped-deployment]
== Cluster Scoped Supporting Services

The `SonataFlowClusterPlatform` CR is optionally used to specify a cluster-wide set of supporting services for workflow consumption.
This is done by referencing an existing, namespaced `SonataFlowPlatform` CR.

Following is a basic configuration that allows workflows, deployed in any namespace, to leverage supporting services deployed in the chosen `example-namespace` namespace.

.Example of a SonataFlowClusterPlatform CR
[source,yaml]
----
apiVersion: sonataflow.org/v1alpha08
kind: SonataFlowClusterPlatform
metadata:
  name: cluster-platform
spec:
  platformRef:
    name: sonataflow-platform-example <1>
    namespace: example-namespace <2>
----

<1> Name of the already installed `SonatataFlowPlatform` CR that configures the supporting services.
<2> Namespace of the already installed `SontataFlowPlatform` CR that configures the supporting services.

[NOTE]
====
These cluster-wide services can be overridden in any namespace, by configuring that namespace's `SonataFlowPlatform.spec.services`.
====

== Conclusions

The {operator_name} extends its scope to manage the lifecycle of the xref:data-index/data-index-core-concepts.adoc[Data Index] and xref:job-services/core-concepts.adoc[Job Service] instances, thus removing the burden on the users and allowing them to focus on the implementation of the workflows.
It takes care also of managing all the configurations to facilitate communication between the workflows and the supporting services.
Additionally, it can manage different persistence options for each service, and advanced configurations.


== Additional resources

* xref:data-index/data-index-core-concepts.adoc[]
* xref:job-services/core-concepts.adoc[Job Service Core Concepts]
* xref:cloud/operator/using-persistence.adoc[Workflow Persistence]
* xref:cloud/operator/configuring-workflow-eventing-system.adoc[Workflow Eventing System]
* xref:cloud/operator/known-issues.adoc[]

include::../../../pages/_common-content/report-issue.adoc[]
