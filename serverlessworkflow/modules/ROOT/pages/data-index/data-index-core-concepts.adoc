= Data Index Core Concepts
:compat-mode!:
// Metadata:
:description: Data Index Service to allow to index and query audit data in {product_name}
:keywords: workflow, serverless, data, dataindex, data-index, index, service
// External pages
:cloud_events_url: https://cloudevents.io/
:graphql_url: https://graphql.org
:vertx_url: https://vertx.io/
:infinispan_url: https://infinispan.org/
:mongo_url: https://www.mongodb.com/
:postgresql_url: https://www.postgresql.org/
:dev_services_url: https://quarkus.io/guides/dev-services
:flyway_quarkus_url: https://quarkus.io/guides/flyway

// Referenced documentation pages
:path_resolution_url: https://quarkus.io/blog/path-resolution-in-quarkus/#defaults

//Common constants
:data_index_ref: Data Index
:workflow_instance: workflow instance
:workflow_instances: {workflow_instance}s

In {product_name} platform there is a dedicated supporting service that stores the data related to the {workflow_instances} and their associated jobs called *{data_index_ref}* service.
This service also provides a GraphQL endpoint allowing users to query that data and perform operations, also known as mutations in GraphQL terms.

The data processed by the {data_index_ref} service is usually received via events. The events consumed can be generated by any workflow or the xref::job-services/core-concepts.adoc[Job service] itself.
This event communication can be configured in different ways as described in the <<data-index-service-communication, {data_index_ref} communication configuration>> section.

The {data_index_ref} service uses Apache Kafka or Knative eventing to consume link:{cloud_events_url}[CloudEvents] messages from workflows.
The event data is indexed and stored in the database for querying via GraphQL. These events contain information about units of work executed for a workflow.
The {data_index_ref} service is at the core of all {product_name} search, insight, and management capabilities.

The {product_name} Data Index Service has the following key attributes:

* Flexible data structure
* Distributable and cloud-ready format
* Message-based communication with workflows (Apache Kafka, Knative, CloudEvents)
* Powerful querying API using GraphQL
* Management capabilities using the Gateway API, to perform actions using GraphQL syntax on remote runtimes with a single entrypoint

== {data_index_ref} service in {product_name}

The {product_name} {data_index_ref} Service is a Quarkus application, based on link:{vertx_url}[Vert.x] with link:{smallrye_messaging_url}[Reactive Messaging], that exposes a link:{graphql_url}[GraphQL] endpoint that client applications use to access indexed data and perform management operations related to running workflow instances.

[NOTE]
====
The indexing functionality in the {data_index_ref} service is provided by choosing one of the following persistence providers:

* link:{postgresql_url}[PostgreSQL]
* link:{infinispan_url}[Infinispan]
* link:{mongo_url}[MongoDB]
====

The {data_index_ref} Service has been thought of as an application to store and query the existing workflow data. The data comes contained in events. The service allows multiple connection options as described in the <<data-index-service-communication, {data_index_ref} service communication configuration>> section.

[#data-index-deployments]
== {data_index_ref} scenarios

{data_index_ref} is distributed in different ways to allow deployment in different topologies, and depending on how the data is indexed.

The following sections describe the different options of {data_index_ref} deployments.

=== {data_index_ref} as a standalone service

It can be deployed explicitly referencing the *image*, starting a separated service inside a container. See xref:data-index/data-index-service.adoc[{data_index_ref} standalone].

image::data-index/data-index-standalone-service.png[Image of data-index deployment an external service]

This type of deployment requires to choose the right image depending on the persistence, specify the database connection properties, and the event consumption configuration.

[#data-index-dev-service]
=== {data_index_ref} service as Quarkus Development service
It also can be deployed, transparently as a *Quarkus Development Service* when the Quarkus Dev mode is used in the {product_name} application.
When you use the {product_name} Process Quarkus extension, a temporary {data_index_ref} Service is automatically provisioned while the Quarkus application is running in development mode and the Dev Service is set up for immediate use.

image::data-index/data-index-dev-service.png[Image of data-index deployment an Quarkus Dev Service]

More details are provided in the xref:data-index/data-index-service.adoc#data-index-dev-service-details[{data_index_ref} as a Quarkus Development service] section.

The {product_name} Process Quarkus extension sets up your Quarkus application to automatically replicate any {product_name} messaging events related to {workflow_instances} or jobs into the provisioned Data Index instance.

For more information about Quarkus Dev Services, see link:{dev_services_url}[Dev Services guide].

=== {data_index_ref} service as Quarkus extension
It can be included as part of the same {product_name} application using the *{data_index_ref} extension*, through the provided addons.

This scenario is specific to add the {data_index_ref} data indexing features and the GraphQL endpoint exposure inside a workflow application.

The communication with the workflow where the extension is added, is something internal to the application, allowing to simplify the communication between services and avoiding extra configuration for that purpose.

In this case, the indexation has some limitations: it is only able to index data from the workflows deployed in the same application.


image::data-index/data-index-addon.png[Image of data-index as a Quarkus Extension]

More details are available in the xref:data-index/data-index-quarkus-extension.adoc[{data_index_ref} Quarkus Extension] section.

[#data-index-storage]
== {data_index_ref} storage

In order to store the indexed data, {data_index_ref} needs some specific tables to be created. {data_index_ref} is ready to use link:{flyway_quarkus_url}[Quarkus flyway] for that purpose.

It's necessary to activate the migrate-at-start option to migrate the {data_index_ref} schema automatically.

For more details about Flyway migrations, see xref:persistence/postgresql-flyway-migration.adoc[] section

[#data-index-graphql]
== {data_index_ref} GraphQL endpoint

{data_index_ref} provides GraphQL endpoint that allows users to interact with the stored data.
For more information about GraphQL see {graphql_url}[GraphQL]

[#data-index-ext-queries]
=== GraphQL queries for {workflow_instances} and jobs

This guide provides as examples, some GraphQL queries that allow to retrieve data about {workflow_instance}s and jobs.

Retrieve data from {workflow_instances}::
+
--
You can retrieve data about a specified instance from your workflow definition.

.Example query
[source]
----
{
  ProcessInstances {
    id
    processId
    state
    parentProcessInstanceId
    rootProcessId
    rootProcessInstanceId
    variables
    nodes {
      id
      name
      type
    }
  }
}
----
--

Retrieve data from jobs::
+
--
You can retrieve data from a specified job instance.

.Example query
[source]
----
{
  Jobs {
    id
    status
    priority
    processId
    processInstanceId
    executionCounter
  }
}
----
--

Filter query results using the `where` parameter::
+
--
You can use the `where` parameter with multiple combinations to filter query results based on workflow attributes.

.Example query
[source]
----
{
  ProcessInstances(where: {state: {equal: ACTIVE}}) {
    id
    processId
    processName
    start
    state
    variables
  }
}
----

.Example query
[source]
----
{
  ProcessInstances(where: {id: {equal: "d43a56b6-fb11-4066-b689-d70386b9a375"}}) {
    id
    processId
    processName
    start
    state
    variables
  }
}
----

By default, all filtered attributes are executed as `AND` operations in queries. You can modify this behavior by combining filters with an `AND` or `OR` operator.

.Example query
[source]
----
{
  ProcessInstances(where: {or: {state: {equal: ACTIVE}, rootProcessId: {isNull: false}}}) {
    id
    processId
    processName
    start
    end
    state
  }
}
----

.Example query
[source]
----
{
  ProcessInstances(where: {and: {processId: {equal: "travels"}, or: {state: {equal: ACTIVE}, rootProcessId: {isNull: false}}}}) {
    id
    processId
    processName
    start
    end
    state
  }
}
----

Depending on the attribute type, the following operators are also available:

* String array argument:
** `contains` : String
** `containsAll`: Array of strings
** `containsAny`: Array of strings
** `isNull`: Boolean (`true` or `false`)

* String argument:
** `in`: Array of strings
** `like`: String
** `isNull`: Boolean (`true` or `false`)
** `equal`: String

* ID argument:
** `in`: Array of strings
** `equal`: String
** `isNull`: Boolean (`true` or `false`)

* Boolean argument:
** `isNull`: Boolean (`true` or `false`)
** `equal`: Boolean (`true` or `false`)

* Numeric argument:
** `in`: Array of integers
** `isNull`: Boolean
** `equal`: Integer
** `greaterThan`: Integer
** `greaterThanEqual`: Integer
** `lessThan`: Integer
** `lessThanEqual`: Integer
** `between`: Numeric range
** `from`: Integer
** `to`: Integer

* Date argument:
** `isNull`: Boolean (`true` or `false`)
** `equal`: Date time
** `greaterThan`: Date time
** `greaterThanEqual`: Date time
** `lessThan`: Date time
** `lessThanEqual`: Date time
** `between`: Date range
** `from`: Date time
** `to`: Date time
--

Sort query results using the `orderBy` parameter::
+
--
You can use the `orderBy` parameter to sort query results based on workflow attributes. You can also specify the direction of sorting in ascending `ASC` order or descending `DESC` order. Multiple attributes are applied to the database query in the order they are specified in the query filter.

.Example query
[source]
----
{
  ProcessInstances(where: {state: {equal: ACTIVE}}, orderBy: {start: ASC}) {
    id
    processId
    processName
    start
    end
    state
  }
}
----
--

Limit and offset query results using the `pagination` parameter::
+
--
You can use the `pagination` parameter to specify a `limit` and `offset` for query results.

.Example query
[source]
----
{
  ProcessInstances(where: {state: {equal: ACTIVE}}, orderBy: {start: ASC}, pagination: {limit: 10, offset: 0}) {
    id
    processId
    processName
    start
    end
    state
  }
}
----
--

[#data-index-gateway-api]
=== Data Index service Gateway API

Data Index incorporates a set of queries or mutations that allow firing operations on workflow endpoints using GraphQL notation.

The Data Index Gateway API enables you to perform the following operations:

Abort a {workflow_instance}::
+
--
Retrieves a {workflow_instance} with the ID passed as a parameter and launches the abort operation on related {product_name} service.

.Example mutation for abort operation
[source]
----
mutation {
    ProcessInstanceAbort (id:"66e05e9c-eaab-47af-a83e-156498b7096d")
}
----
--

Retry a {workflow_instance}::
+
--
Retrieves a {workflow_instance} with the id passed as a parameter and launches the retry operation on related {product_name} service.

.Example mutation for retry operation
[source]
----
mutation {
    ProcessInstanceRetry (id:"66e05e9c-eaab-47af-a83e-156498b7096d")
}
----
--

Skip a {workflow_instance}::
+
--
Retrieves a {workflow_instance} with the ID passed as a parameter and launches the skip operation on related {product_name} service.

.Example mutation for skip operation
[source]
----
mutation {
    ProcessInstanceSkip (id:"66e05e9c-eaab-47af-a83e-156498b7096d")
}
----
--


Retrieve {workflow_instance} nodes::
+
--
Retrieves the nodes of a {workflow_instance} that are coming from the process definition. When the `nodeDefinitions` field of a {workflow_instance} is queried, a call to a specific {product_name} service is generated to retrieve the requested list of available nodes.

.Example query to retrieve {workflow_instance} nodes
[source]
----
{ProcessInstances(where: { id: {equal: "1017afb1-5749-440e-8b9b-6b876bb5894d"}}){
  diagram
}}
----
--

Update {workflow_instance} variables::
+
--
Updates the variables of a {workflow_instance} using the `id` passed as a parameter. Retrieves a {workflow_instance} using the `id` passed as a parameter and launches the update operation on related {product_name} service with the new values passed in `variables` parameter.

.Example mutation to update {workflow_instance} variables
[source]
----
mutation {
    ProcessInstanceUpdateVariables
        (id:"23147fcc-da82-43a2-a577-7a36b26094bd",
         variables:"{\"it_approval\":true,\"candidate\":{\"name\":\"Joe\",\"email\":\"jdoe@ts.com\",\"salary\":30000,\"skills\":\"java\"},\"hr_approval\":true}")
}
----
--

Trigger a node instance::
+
--
Triggers a node instance using the node definition `nodeId`. The `nodeId` is included in the `nodeInstances` of a {workflow_instance} using the `id` passed as parameter.

.Example mutation to trigger a node instance
[source]
----
mutation{
  NodeInstanceTrigger(
    id: "9674e3ed-8c13-4c3f-870e-7465d9ca7ca7",
    nodeId:"_B8C4F63C-81AD-4291-9C1B-84967277EEF6")
}
----
--

Retrigger a node instance::
+
--
Retriggers a node instance using the `id`, which is similar to `nodeInstanceId` related to a {workflow_instance}. The `id` of the {workflow_instance} is passed as a parameter.

.Example mutation to retrigger a node instance
[source]
----
mutation{
  NodeInstanceRetrigger(
    id: "9674e3ed-8c13-4c3f-870e-7465d9ca7ca7",
    nodeInstanceId:"01756ba2-ac16-4cf1-9d74-154ae8f2df21")
}
----
--

Cancel a node instance::
+
--
Cancels a node instance with the `id`, which is similar to `nodeInstanceId` related to a {workflow_instance}. The `id` of the {workflow_instance} is passed as a parameter.

.Example mutation to cancel a node instance
[source]
----
mutation{
  NodeInstanceCancel(
    id: "9674e3ed-8c13-4c3f-870e-7465d9ca7ca7",
    nodeInstanceId:"01756ba2-ac16-4cf1-9d74-154ae8f2df21")
}
----


[NOTE]
====
To enable described management operations on workflow instances, make sure your project is configured to have the `kogito-addons-quarkus-process-management` dependency on its `pom.xml` file to have this management operations enabled, like:
[source,xml]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-quarkus-process-management</artifactId>
</dependency>
----
====
--

Retrieve the {workflow_instance} source file content::
+
--
Retrieves the {workflow_instance} source file. When the `source` field of a {workflow_instance} is queried, a call to a specific {product_name} service is generated to retrieve the requested {workflow_instance} source file content.

.Example query to retrieve a {workflow_instance} source file content
[source]
----
{ProcessInstances(where: { id: {equal: "1017afb1-5749-440e-8b9b-6b876bb5894d"}}){
  source
}}
----

[NOTE]
====
The workflow instance source field only will be available when `kogito-addons-quarkus-source-files` dependency is added on {product_name} runtime service `pom.xml` file.
[source,xml]
----
    <dependency>
      <groupId>org.kie.kogito</groupId>
      <artifactId>kogito-addons-quarkus-source-files</artifactId>
    </dependency>
----
====
--

Reschedule a job::
+
--
Reschedules a job using the `id`. The job `id` and other information are passed in the `data` parameter.

.Example mutation to reschedule a job
[source]
----
mutation{
  JobReschedule(
    id: "9674e3ed-8c13-4c3f-870e-7465d9ca7ca7",
    data:"{\"expirationTime\": \"2033-06-27T04:35:54.631Z\",\"retries\": 2}")
}
----
--

Cancel a job::
+
--
Cancels a job using the `id` passed as a parameter.

.Example mutation to cancel a job
[source]
----
mutation{
  JobCancel(
    id: "9674e3ed-8c13-4c3f-870e-7465d9ca7ca7")
}
----
--

[#data-index-graphql-ui]
=== {data_index_ref} GraphQL UI

Data Index GraphQL UI is provided to interact with GraphQL endpoint.

image::data-index/data-index-graphql-ui.png[Image of data-index GraphQL UI]

When the {data_index_ref} is deployed as a standalone service, this UI will be available at `/graphiql/` endpoint (i.e: at http://localhost:8180/graphiql/)

To have the GraphQL UI available when the {data_index_ref} extension is deployed the property `quarkus.kogito.data-index.graphql.ui.always-include` needs to be enabled.

It will be accessible at: <quarkus.http.root-path><quarkus.http.non-application-root-path>/graphql-ui/ (i.e: http://localhost:8080/q/graphql-ui/)

[NOTE]
====
The `quarkus.http.root-path' and `quarkus.http.non-application-root-path` belong to the workflow application where the {data_index_ref} extension has been added. {path_resolution_url}[Here] there are more details about those properties and their default values.
====

[#data-index-service-communication]
== {data_index_ref} service communication configuration

In order to index the data, {data_index_ref} allows multiple connection options to be able to consume the information provided by the different workflows.

The final goal is to receive the application-generated data related to the {workflow_instances} and jobs. The information that comes inside events, is indexed and stored in the database allowing it to be consumed through the provided GraphQL endpoint.

=== Knative Eventing

In order to interact with the {data_index_ref} separated service, use the Knative eventing system eventing:

* Add the {data_index_ref} service and deployment, defining the Database connection properties and setting the `KOGITO_DATA_INDEX_QUARKUS_PROFILE` to `http-events-support`.
* Specify the Knative Triggers to filter the {data_index_ref} events.

.Example `DataIndex` resource with triggers definition (requires Knative):
[source,yaml]
----
apiVersion: eventing.knative.dev/v1
kind: Trigger
metadata:
  name: data-index-service-postgresql-processes-trigger
spec:
  broker: default
  filter:
    attributes:
      type: ProcessInstanceEvent
  subscriber:
    ref:
      apiVersion: v1
      kind: Service
      name: data-index-service-postgresql
    uri: /processes
---
apiVersion: eventing.knative.dev/v1
kind: Trigger
metadata:
  name: data-index-service-postgresql-jobs-trigger
spec:
  broker: default
  filter:
    attributes:
      type: JobEvent
  subscriber:
    ref:
      apiVersion: v1
      kind: Service
      name: data-index-service-postgresql
    uri: /jobs
----


* Configure the workflow to use the K_SINK as outgoing connection url

.Example of configuration in {product_name} application `application.properties` file to communicate with Knative
[source,properties]
----
mp.messaging.outgoing.kogito-processinstances-events.connector=quarkus-http
mp.messaging.outgoing.kogito-processinstances-events.url=${K_SINK}
mp.messaging.outgoing.kogito-processinstances-events.method=POST
----

[NOTE]
====
*Job service* needs also to be configured to send the events to the Knative K_SINK to have them available for {data_index_ref} related triggers.
====

=== Kafka eventing

To configure the communication between the {data_index_ref} Service and the workflow through Kafka, you must provide a set of configurations.

* Add the {data_index_ref} service and deployment, defining the Database connection properties and setting the `KOGITO_DATA_INDEX_QUARKUS_PROFILE` to `kafka-events-support` (this value is set by default).

* Configure the {product_name} application to use the smallrye-kafka connector and the expected topic.

.Example of configuration in {product_name} application `application.properties` file to communicate with Kafka
[source,properties]
----
mp.messaging.outgoing.kogito-processinstances-events.connector=smallrye-kafka
mp.messaging.outgoing.kogito-processinstances-events.topic=kogito-processinstances-events
mp.messaging.outgoing.kogito-processinstances-events.value.serializer=org.apache.kafka.common.serialization.StringSerializer
----

[NOTE]
====
*Job service* is configured to send the JobEvents to the kafka topic `kogito-jobs-events` to have them available for {data_index_ref} consumption.
====

=== {data_index_ref} Quarkus extension and Jobs embedded addon
When {data_index_ref} functionality is added as a Quarkus extension to the workflow, there is no event configuration.
In this case, the data indexation is done internally, and all interactions are through the {data_index_ref} Gateway API.

== Additional resources

* xref:eventing/consume-producing-events-with-kafka.adoc[]
* xref:eventing/consume-produce-events-with-knative-eventing.adoc[]
* xref:use-cases/timeout-showcase-example.adoc[]

include::../_common-content/report-issue.adoc[]